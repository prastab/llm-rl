{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## First, check to see if lightning is installed, if not, install it.\nimport pip\ntry:\n  __import__(\"lightning\")\nexcept ImportError:\n  pip.main(['install', \"lightning\"])  \n\nimport torch ## torch let's us create tensors and also provides helper functions\nimport torch.nn as nn ## torch.nn gives us nn.Module(), nn.Embedding() and nn.Linear()\nimport torch.nn.functional as F # This gives us the softmax() and argmax()\nfrom torch.optim import Adam ## We will use the Adam optimizer, which is, essentially, \n                             ## a slightly less stochastic version of stochastic gradient descent.\nfrom torch.utils.data import TensorDataset, DataLoader ## We'll store our data in DataLoaders\n\nimport lightning as L ## Lightning makes it easier to write, optimize and scale our code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:23:18.500986Z","iopub.execute_input":"2025-03-20T18:23:18.501243Z","iopub.status.idle":"2025-03-20T18:23:32.119454Z","shell.execute_reply.started":"2025-03-20T18:23:18.501212Z","shell.execute_reply":"2025-03-20T18:23:32.118789Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n  warnings.warn(\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Collecting lightning\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Collecting lightning\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  Downloading lightning-2.5.1-py3-none-any.whl.metadata (39 kB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Downloading lightning-2.5.1-py3-none-any.whl.metadata (39 kB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: PyYAML&lt;8.0,&gt;=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.12.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: fsspec&lt;2026.0,&gt;=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]&lt;2026.0,&gt;=2022.5.0-&gt;lightning) (2024.12.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.12.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: lightning-utilities&lt;2.0,&gt;=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.12.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: packaging&lt;25.0,&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.1+cu121)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: torch&lt;4.0,&gt;=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.1+cu121)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.1)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: torchmetrics&lt;3.0,&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.1)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: tqdm&lt;6.0,&gt;=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: typing-extensions&lt;6.0,&gt;=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.0.post0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.0.post0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.12)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]&lt;2026.0,&gt;=2022.5.0-&gt;lightning) (3.11.12)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities&lt;2.0,&gt;=0.10.0-&gt;lightning) (75.1.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.17.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch&lt;4.0,&gt;=2.1.0-&gt;lightning) (3.17.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch&lt;4.0,&gt;=2.1.0-&gt;lightning) (3.4.2)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&lt;4.0,&gt;=2.1.0-&gt;lightning) (3.1.4)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch&lt;4.0,&gt;=2.1.0-&gt;lightning) (1.13.1)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1-&gt;torch&lt;4.0,&gt;=2.1.0-&gt;lightning) (1.3.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: numpy&gt;1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics&lt;3.0,&gt;=0.7.0-&gt;lightning) (1.26.4)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.6)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;2026.0,&gt;=2022.5.0-&gt;lightning) (2.4.6)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;2026.0,&gt;=2022.5.0-&gt;lightning) (1.3.2)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (5.0.1)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: async-timeout&lt;6.0,&gt;=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;2026.0,&gt;=2022.5.0-&gt;lightning) (5.0.1)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (25.1.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;2026.0,&gt;=2022.5.0-&gt;lightning) (25.1.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;2026.0,&gt;=2022.5.0-&gt;lightning) (1.5.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;2026.0,&gt;=2022.5.0-&gt;lightning) (6.1.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;2026.0,&gt;=2022.5.0-&gt;lightning) (0.2.1)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;2026.0,&gt;=2022.5.0-&gt;lightning) (1.18.3)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.3.8)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy&gt;1.20.0-&gt;torchmetrics&lt;3.0,&gt;=0.7.0-&gt;lightning) (1.3.8)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.4)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy&gt;1.20.0-&gt;torchmetrics&lt;3.0,&gt;=0.7.0-&gt;lightning) (1.2.4)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (0.1.1)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy&gt;1.20.0-&gt;torchmetrics&lt;3.0,&gt;=0.7.0-&gt;lightning) (0.1.1)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2025.0.1)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy&gt;1.20.0-&gt;torchmetrics&lt;3.0,&gt;=0.7.0-&gt;lightning) (2025.0.1)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy&gt;1.20.0-&gt;torchmetrics&lt;3.0,&gt;=0.7.0-&gt;lightning) (2022.0.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2.4.1)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy&gt;1.20.0-&gt;torchmetrics&lt;3.0,&gt;=0.7.0-&gt;lightning) (2.4.1)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch&lt;4.0,&gt;=2.1.0-&gt;lightning) (3.0.2)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: idna&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl&lt;2.0,&gt;=1.17.0-&gt;aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;2026.0,&gt;=2022.5.0-&gt;lightning) (3.10)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: intel-openmp&gt;=2024 in /usr/local/lib/python3.10/dist-packages (from mkl-&gt;numpy&gt;1.20.0-&gt;torchmetrics&lt;3.0,&gt;=0.7.0-&gt;lightning) (2024.2.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl-&gt;numpy&gt;1.20.0-&gt;torchmetrics&lt;3.0,&gt;=0.7.0-&gt;lightning) (2022.0.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*-&gt;mkl-&gt;numpy&gt;1.20.0-&gt;torchmetrics&lt;3.0,&gt;=0.7.0-&gt;lightning) (1.2.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath-&gt;numpy&gt;1.20.0-&gt;torchmetrics&lt;3.0,&gt;=0.7.0-&gt;lightning) (2024.2.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp&gt;=2024-&gt;mkl-&gt;numpy&gt;1.20.0-&gt;torchmetrics&lt;3.0,&gt;=0.7.0-&gt;lightning) (2024.2.0)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading lightning-2.5.1-py3-none-any.whl (818 kB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloading lightning-2.5.1-py3-none-any.whl (818 kB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f35be30d9def4aac9b58bc17afbc3c24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Installing collected packages: lightning\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Installing collected packages: lightning\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Successfully installed lightning-2.5.1\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Successfully installed lightning-2.5.1\n</pre>\n"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from typing import List\nimport random\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom typing import List\nimport random\n\ndef generate_synthetic_data(num_data: int, max_digits: int) -> List[List[int]]:\n    \"\"\"\n    Generate synthetic data for training the language model.\n\n    Args: \n        num_data (int): Number of data points to generate.\n        max_digits (int): Maximum number of digits (not directly used; kept for compatibility).\n\n    Returns:\n        List[List[int]]: List of sequences, where each sequence is a list of token IDs.\n    \"\"\"\n    # Define the token-to-ID mapping\n    token_to_id = {\n        '0': 0, '1': 1, '2': 2, '3': 3, '4': 4,\n        '5': 5, '6': 6, '7': 7, '8': 8, '9': 9,\n        '.': 10,\n        'or': 11,\n        '<': 12,\n        '>': 13,\n        '=': 14,\n        '[BOS]': 15,\n        '[EOS]': 16,\n        '[SEP]': 17,\n        '[PAD]': 18\n    }\n\n    def generate_number():\n        # Integer part: 0 to 9999 ensures up to 4 digits\n        int_part = str(random.randint(0, 999))\n        # Decide whether to include a decimal part (50% chance)\n        if random.random() < 0.7:\n            # Decimal digits: 1 or 2\n            dec_digits = random.randint(1, 2)\n            dec_part = ''.join(str(random.randint(0, 9)) for _ in range(dec_digits))\n            return int_part + '.' + dec_part\n        return int_part\n\n    sequences = []\n    for _ in range(num_data):\n        # Generate two numbers\n        num1_str = generate_number()\n        num2_str = generate_number()\n        \n        # Convert to floats for comparison\n        num1_float = float(num1_str)\n        num2_float = float(num2_str)\n        \n        # Determine the operator\n        if num1_float < num2_float:\n            operator = '<'\n        elif num1_float > num2_float:\n            operator = '>'\n        else:\n            operator = '='\n        \n        # Construct the sequence\n        input_part = f\"[BOS]{num1_str}or{num2_str}[SEP]\"\n        output_part = f\"{num1_str}{operator}{num2_str}[EOS]\"\n        sequence_str = input_part + output_part\n        \n        # Tokenize the sequence\n        sequence_tokens = []\n        i = 0\n        while i < len(sequence_str):\n            if sequence_str[i:i+5] in ('[BOS]', '[EOS]', '[SEP]'):\n                sequence_tokens.append(sequence_str[i:i+5])\n                i += 5\n            elif sequence_str[i:i+2] == 'or':\n                sequence_tokens.append('or')\n                i += 2\n            else:\n                sequence_tokens.append(sequence_str[i])\n                i += 1\n        \n        # Map tokens to IDs\n        sequence_ids = [token_to_id[token] for token in sequence_tokens]\n        sequences.append(sequence_ids)\n\n    return sequences\ntoken_to_id = {\n        '0': 0, '1': 1, '2': 2, '3': 3, '4': 4,\n        '5': 5, '6': 6, '7': 7, '8': 8, '9': 9,\n        '.': 10,\n        'or': 11,\n        '<': 12,\n        '>': 13,\n        '=': 14,\n        '[BOS]': 15,\n        '[EOS]': 16,\n        '[SEP]': 17,\n        '[PAD]': 18\n    }\n\nid_to_token = dict(map(reversed, token_to_id.items()))\n\ndata = generate_synthetic_data(num_data=100000, max_digits=2)\n\nprint(data[0])\nprint(\"Generated sequence (text):\", \"\".join([id_to_token[token_id] for token_id in data[0]]))\n\n# Convert to tensors\ndata_tensors = [torch.tensor(seq, dtype=torch.long) for seq in data]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:24:05.402060Z","iopub.execute_input":"2025-03-20T18:24:05.402387Z","iopub.status.idle":"2025-03-20T18:24:08.279579Z","shell.execute_reply.started":"2025-03-20T18:24:05.402362Z","shell.execute_reply":"2025-03-20T18:24:08.278703Z"}},"outputs":[{"name":"stdout","text":"[15, 3, 9, 1, 10, 5, 6, 11, 3, 4, 5, 17, 3, 9, 1, 10, 5, 6, 13, 3, 4, 5, 16]\nGenerated sequence (text): [BOS]391.56or345[SEP]391.56>345[EOS]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"train_size = int(0.9 * len(data_tensors))  # 90% for training\nval_size = len(data_tensors) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(data_tensors, [train_size, val_size])\n\ndef collate_fn(batch):\n    return pad_sequence(batch, batch_first=True, padding_value=0)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)\nval_dataloader = DataLoader(val_dataset, batch_size=1024, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:24:08.280623Z","iopub.execute_input":"2025-03-20T18:24:08.280898Z","iopub.status.idle":"2025-03-20T18:24:08.304296Z","shell.execute_reply.started":"2025-03-20T18:24:08.280877Z","shell.execute_reply":"2025-03-20T18:24:08.303655Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class PositionEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 50):\n        super().__init__()\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n        pe = torch.zeros(1, max_len, d_model)  # Shape: (1, max_len, d_model) for batch-first\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x: (batch_size, seq_len, d_model)\n        x = x + self.pe[:, :x.size(1), :]  # Slice to seq_len, broadcast over batch\n        return x\n\nclass Attention(nn.Module):\n    def __init__(self, d_model: int):\n        super().__init__()\n        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n        self.d_model = d_model\n\n    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n        q = self.W_q(encodings_for_q)\n        k = self.W_k(encodings_for_k)\n        v = self.W_v(encodings_for_v)\n        \n        # Scaled dot-product attention\n        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n\n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        output = torch.matmul(attn_probs, v)\n        return output\n\n\nclass DecoderLayer(nn.Module):\n  def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n      super().__init__()\n      self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n      self.norm1 = nn.LayerNorm(d_model)\n      self.ff = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Linear(d_ff, d_model)\n          )\n      self.norm2 = nn.LayerNorm(d_model)\n      self.dropout = nn.Dropout(dropout)\n\n\n  def forward(self, x, mask):\n    # Self-attention with residual connection and layer normalization\n    attn_output, _ = self.self_attn(x, x, x, attn_mask=mask)\n    x = self.norm1(x + self.dropout(attn_output))\n\n    # Feedforward network with residual and layer normalization.\n    ff_output = self.ff(x)\n    x = self.norm2(x + self.dropout(ff_output))\n\n    return x\n    \n\nclass DecoderOnlyTransformer(L.LightningModule):\n    def __init__(self, num_tokens: int = 19, d_model: int = 128, num_heads: int = 4, num_layers: int = 3, d_ff: int = 512, max_len: int = 100, dropout=0.1):\n        super().__init__()\n        L.seed_everything(seed=42)\n        self.embedding = nn.Embedding(num_tokens, d_model)\n        self.pos_encoding = PositionEncoding(d_model, max_len)\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.fc_out = nn.Linear(d_model, num_tokens)\n        self.dropout = nn.Dropout(dropout)\n        self.loss = nn.CrossEntropyLoss(ignore_index=18)\n        self.d_model = d_model\n\n    def forward(self, x, mask):\n      x = self.embedding(x) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32)) # Scale embeddings\n      x = self.pos_encoding(x)\n      x = self.dropout(x)\n      for layer in self.decoder_layers:\n          x = layer(x, mask)\n      x = self.fc_out(x)\n      return x\n\n\n    def configure_optimizers(self):\n        return Adam(self.parameters(), lr=3e-4)\n\n    def training_step(self, batch, batch_idx):\n        inputs = batch[:, :-1]\n        labels = batch[:, 1:]\n        mask = self.generate_square_subsequent_mask(inputs.size(1)).to(inputs.device)\n        output = self.forward(inputs, mask)\n        output = output.view(-1, output.size(-1))\n        labels = labels.contiguous().view(-1)\n        loss = self.loss(output, labels)\n        self.log('train_loss', loss)\n        # print(\"Batch:\", batch_idx)\n        # print(\"Loss: \", loss.item())\n        return loss\n    def validation_step(self, batch, batch_idx):\n        inputs = batch[:, :-1]\n        labels = batch[:, 1:]\n        mask = self.generate_square_subsequent_mask(inputs.size(1)).to(inputs.device)\n        output = self.forward(inputs, mask)\n        output = output.view(-1, output.size(-1))\n        labels_flat = labels.contiguous().view(-1)\n        loss = self.loss(output, labels_flat)\n        predictions = torch.argmax(output, dim=-1)\n        correct_predictions = (predictions == labels_flat)\n        non_padding_mask = (labels_flat != 0)\n        accuracy = (correct_predictions & non_padding_mask).float().sum() / non_padding_mask.float().sum()\n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('val_acc', accuracy, on_step=False, on_epoch=True, prog_bar=True)\n        # print(\"Validation Loss: \", loss.item())\n        # print(\"Validation Accuracy:\", accuracy.item())\n        return loss\n\n    def generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n        \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n        Unmasked positions are filled with float(0.0).\n        \"\"\"\n        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n\n    def generate(self, input_sequence, max_length=20):\n      self.eval()  # Set the model to evaluation mode\n      with torch.no_grad():  # Disable gradient calculation\n          input_tensor = torch.tensor(input_sequence, dtype=torch.long).unsqueeze(0).to(self.device) # Add batch dimension and move to device\n          output = input_tensor\n          for _ in range(max_length):\n                mask = self.generate_square_subsequent_mask(output.size(1)).to(self.device)\n                predictions = self.forward(output, mask)\n                # Get the last predicted token\n                next_token_prediction = predictions[:, -1, :]\n                # Get the most likely next token\n                next_token = torch.argmax(next_token_prediction, dim=-1)\n                # Concatenate the next token to the output sequence\n                output = torch.cat([output, next_token.unsqueeze(0)], dim=1)\n\n                # If the next token is [EOS], stop generating\n                if next_token.item() == 16: # [EOS] token ID\n                    break\n      return output.squeeze(0).tolist() # remove the batch dimension","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:24:08.305596Z","iopub.execute_input":"2025-03-20T18:24:08.305882Z","iopub.status.idle":"2025-03-20T18:24:08.324454Z","shell.execute_reply.started":"2025-03-20T18:24:08.305862Z","shell.execute_reply":"2025-03-20T18:24:08.323634Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\nnum_tokens = 19\nd_model = 32\nnum_heads = 4\nnum_layers = 8\nd_ff = 1024\nmax_len = 50\ndropout = 0.1\n\n# --- Model Creation ---\nmodel = DecoderOnlyTransformer(num_tokens, d_model, num_heads, num_layers, d_ff, max_len, dropout)\n\n# --- Training (using Lightning) ---\ntrainer = L.Trainer(\n    max_epochs=10,  # More epochs\n    accelerator=\"gpu\",\n    devices=\"auto\",\n    gradient_clip_val=0.5, \n    # accumulate_grad_batches = 2 # Add gradient clipping\n    # profiler=\"simple\", # uncomment for profiling\n)\ntrainer.fit(model, train_dataloader, val_dataloader)  \ntrainer.save_checkpoint(\"/kaggle/working/base.ckpt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:24:08.950360Z","iopub.execute_input":"2025-03-20T18:24:08.950650Z","iopub.status.idle":"2025-03-20T18:25:41.466450Z","shell.execute_reply.started":"2025-03-20T18:24:08.950624Z","shell.execute_reply":"2025-03-20T18:25:41.465720Z"}},"outputs":[{"name":"stderr","text":"INFO: Seed set to 42\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Seed set to 42\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Seed set to 42\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"GPU available: True (cuda), used: True\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">GPU available: True (cuda), used: True\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"INFO: TPU available: False, using: 0 TPU cores\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"TPU available: False, using: 0 TPU cores\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TPU available: False, using: 0 TPU cores\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"INFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HPU available: False, using: 0 HPUs\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HPU available: False, using: 0 HPUs\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"NumExpr defaulting to 4 threads.\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">NumExpr defaulting to 4 threads.\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"INFO: \n  | Name           | Type             | Params | Mode \n------------------------------------------------------------\n0 | embedding      | Embedding        | 608    | train\n1 | pos_encoding   | PositionEncoding | 0      | train\n2 | decoder_layers | ModuleList       | 567 K  | train\n3 | fc_out         | Linear           | 627    | train\n4 | dropout        | Dropout          | 0      | train\n5 | loss           | CrossEntropyLoss | 0      | train\n------------------------------------------------------------\n568 K     Trainable params\n0         Non-trainable params\n568 K     Total params\n2.275     Total estimated model params size (MB)\n86        Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\n  | Name           | Type             | Params | Mode \n------------------------------------------------------------\n0 | embedding      | Embedding        | 608    | train\n1 | pos_encoding   | PositionEncoding | 0      | train\n2 | decoder_layers | ModuleList       | 567 K  | train\n3 | fc_out         | Linear           | 627    | train\n4 | dropout        | Dropout          | 0      | train\n5 | loss           | CrossEntropyLoss | 0      | train\n------------------------------------------------------------\n568 K     Trainable params\n0         Non-trainable params\n568 K     Total params\n2.275     Total estimated model params size (MB)\n86        Modules in train mode\n0         Modules in eval mode\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n  | Name           | Type             | Params | Mode \n------------------------------------------------------------\n0 | embedding      | Embedding        | 608    | train\n1 | pos_encoding   | PositionEncoding | 0      | train\n2 | decoder_layers | ModuleList       | 567 K  | train\n3 | fc_out         | Linear           | 627    | train\n4 | dropout        | Dropout          | 0      | train\n5 | loss           | CrossEntropyLoss | 0      | train\n------------------------------------------------------------\n568 K     Trainable params\n0         Non-trainable params\n568 K     Total params\n2.275     Total estimated model params size (MB)\n86        Modules in train mode\n0         Modules in eval mode\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33a556815c5c42f4aa5e60be1255aaf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"`Trainer.fit` stopped: `max_epochs=10` reached.\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">`Trainer.fit` stopped: `max_epochs=10` reached.\n</pre>\n"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# checkpoint_path = \"/kaggle/working/start2.ckpt\" \n\n# num_tokens = 19\n# d_model = 64\n# num_heads = 8\n# num_layers = 8\n# d_ff = 2048\n# max_len = 50\n# dropout = 0.1\n\n# model = DecoderOnlyTransformer.load_from_checkpoint(\n#     checkpoint_path,\n#     num_tokens=num_tokens,  # Pass hyperparameters to reconstruct the model\n#     d_model=d_model,\n#     num_heads=num_heads,\n#     num_layers=num_layers,\n#     d_ff=d_ff,\n#     max_len=max_len,\n#     dropout=dropout\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## New RL Method with log(p) and log(1-p)","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nclass ImprovedRLFineTuner:\n    def __init__(self, model, learning_rate=1e-5, gamma=0.99):\n        \"\"\"Initialize the improved RL fine-tuner.\"\"\"\n        self.model = model\n        self.model.train()\n        \n        # Use a smaller learning rate for stability\n        self.optimizer = Adam(self.model.parameters(), lr=learning_rate)\n        self.gamma = gamma\n        \n        # Create token mappings for easier reference\n        self.id_to_token = {\n            0: '0', 1: '1', 2: '2', 3: '3', 4: '4',\n            5: '5', 6: '6', 7: '7', 8: '8', 9: '9',\n            10: '.', 11: 'or', 12: '<', 13: '>', \n            14: '=', 15: '[BOS]', 16: '[EOS]', 17: '[SEP]', 18: '[PAD]'\n        }\n        \n        # Track metrics\n        self.rewards_history = []\n        self.accuracy_history = []\n    \n    def parse_numbers(self, sequence):\n        \"\"\"Extract the two numbers from a sequence for comparison.\"\"\"\n        # Convert sequence to string\n        if isinstance(sequence, list) or isinstance(sequence, torch.Tensor):\n            sequence = \"\".join([self.id_to_token.get(int(token), \"\") for token in sequence])\n        \n        # Find the position of 'or'\n        or_pos = sequence.find('or')\n        if or_pos == -1:\n            return None, None\n        \n        # Find the position of [BOS] and [SEP]\n        bos_pos = sequence.find('[BOS]')\n        sep_pos = sequence.find('[SEP]')\n        \n        if bos_pos == -1 or sep_pos == -1:\n            return None, None\n        \n        # Extract the numbers\n        num1_str = sequence[bos_pos+5:or_pos]\n        num2_str = sequence[or_pos+2:sep_pos]\n        \n        try:\n            num1 = float(num1_str)\n            num2 = float(num2_str)\n            return num1, num2\n        except ValueError:\n            return None, None\n    \n    def compute_reward(self, generated_sequence, input_sequence):\n        \"\"\"Compute reward based on correct comparison operator.\"\"\"\n        # Parse numbers from input\n        num1, num2 = self.parse_numbers(input_sequence)\n        if num1 is None or num2 is None:\n            return -1  # Invalid input\n        \n        # Determine correct operator\n        correct_op = '<' if num1 < num2 else '>' if num1 > num2 else '='\n        correct_op_id = 12 if correct_op == '<' else 13 if correct_op == '>' else 14\n        \n        # Find operator in generated sequence\n        generated_op = None\n        for token in generated_sequence:\n            if token in [12, 13, 14]:  # < or > or =\n                generated_op = token\n                break\n        \n        # No operator found - large penalty\n        if generated_op is None:\n            return -1\n        \n        # Compare with correct operator\n        if generated_op == correct_op_id:\n            return 1.0  # Correct comparison\n        else:\n            return -1.0  # Incorrect comparison\n    \n    def train_one_epoch(self, dataloader):\n        \"\"\"Train for one epoch using both correct and incorrect examples with different weights.\"\"\"\n        total_reward = 0\n        total_correct = 0\n        total_samples = 0\n        \n        self.model.train()\n        \n        # Process batches\n        for batch in tqdm(dataloader, desc=\"Training\"):\n            batch = batch.to(self.model.device)\n            batch_size = batch.size(0)\n            \n            for i in range(batch_size):\n                # Find the separator token ([SEP]) position\n                sep_positions = (batch[i] == 17).nonzero(as_tuple=True)[0]\n                if len(sep_positions) == 0:\n                    continue  # Skip if no separator\n                    \n                sep_position = sep_positions[0].item()\n                \n                # Input is everything up to and including [SEP]\n                input_seq = batch[i, :sep_position+1].unsqueeze(0)\n                \n                # 1. Generate the sequence with the current model\n                with torch.no_grad():\n                    generated = self.model.generate(input_seq[0].tolist())\n                \n                # 2. Compute reward\n                reward = self.compute_reward(generated, batch[i].tolist())\n                total_reward += reward\n                \n                # Check if prediction is correct\n                is_correct = reward > 0\n                if is_correct:\n                    total_correct += 1\n                total_samples += 1\n                \n                # 3. Create a modified target sequence with the correct operator\n                num1, num2 = self.parse_numbers(batch[i].tolist())\n                \n                if num1 is not None and num2 is not None:\n                    # Determine correct operator\n                    correct_op_id = 12 if num1 < num2 else 13 if num1 > num2 else 14\n                    \n                    # Extract parts before and after the operator from the target\n                    target_parts = []\n                    op_found = False\n                    \n                    for token in batch[i].tolist():\n                        if token in [12, 13, 14]:  # Found an operator\n                            target_parts.append(correct_op_id)  # Replace with correct operator\n                            op_found = True\n                        else:\n                            target_parts.append(token)\n                    \n                    if not op_found:\n                        continue  # Skip if no operator in target\n                    \n                    # Create target tensor\n                    target = torch.tensor(target_parts, device=self.model.device).unsqueeze(0)\n                    \n                    # 4. Compute loss with this target and update\n                    # Create inputs for the model (everything except the last token)\n                    inputs = target[:, :-1]\n                    # Create targets (everything except the first token - shifted by 1)\n                    targets = target[:, 1:]\n                    \n                    # Create appropriate mask\n                    mask = self.model.generate_square_subsequent_mask(inputs.size(1)).to(self.model.device)\n                    \n                    # Forward pass\n                    output = self.model(inputs, mask)\n                    \n                    # Custom loss computation\n                    epsilon = 1e-8  # Small value to prevent log(0)\n                    p = torch.softmax(output, dim=-1)  # [1, seq_len_inputs, vocab_size], probabilities\n                    total_neg_log_one_minus_p = (-torch.log(1 - p + epsilon)).sum(dim=-1)  # [1, seq_len_inputs], sum over vocab\n                    p_ct = p.gather(dim=-1, index=targets.unsqueeze(-1)).squeeze(-1)  # [1, seq_len_inputs], prob of correct tokens\n                    neg_log_p_ct = -torch.log(p_ct + epsilon)  # [1, seq_len_inputs], -log(p) for correct tokens\n                    log_one_minus_p_ct = torch.log(1 - p_ct + epsilon)  # [1, seq_len_inputs], log(1 - p) for correct tokens\n                    loss_per_position = neg_log_p_ct + total_neg_log_one_minus_p + log_one_minus_p_ct  # Custom loss per position\n                    valid_mask = (targets != 18)  # [1, seq_len_inputs], mask out padding tokens\n                    if valid_mask.sum() == 0:\n                        continue  # Skip if all tokens are padding\n                    loss = loss_per_position[valid_mask].mean()  # Average over valid positions\n                    \n                    # Apply different scaling based on whether prediction was correct\n                    if is_correct:\n                        # For correct predictions, use a smaller weight\n                        loss = loss * 0.2  # 5x smaller weight than incorrect predictions\n                    else:\n                        # For incorrect predictions, use a larger weight\n                        loss = loss * abs(reward)  # Full weight based on reward magnitude\n                    \n                    # Update\n                    self.optimizer.zero_grad()\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                    self.optimizer.step()\n        \n        # Calculate statistics\n        avg_reward = total_reward / max(total_samples, 1)\n        accuracy = total_correct / max(total_samples, 1)\n        \n        self.rewards_history.append(avg_reward)\n        self.accuracy_history.append(accuracy)\n        \n        print(f\"Epoch stats - Avg Reward: {avg_reward:.4f}, Accuracy: {accuracy:.4f}\")\n        return avg_reward, accuracy\n    \n    def evaluate(self, dataloader):\n        \"\"\"Evaluate the model on a validation set.\"\"\"\n        self.model.eval()\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(dataloader, desc=\"Evaluating\"):\n                batch = batch.to(self.model.device)\n                \n                for i in range(batch.size(0)):\n                    # Find separator position\n                    sep_positions = (batch[i] == 17).nonzero(as_tuple=True)[0]\n                    if len(sep_positions) == 0:\n                        continue\n                        \n                    sep_position = sep_positions[0].item()\n                    \n                    # Extract input sequence\n                    input_seq = batch[i, :sep_position+1].tolist()\n                    \n                    # Extract numbers for comparison\n                    num1, num2 = self.parse_numbers(batch[i].tolist())\n                    if num1 is None or num2 is None:\n                        continue\n                        \n                    # Determine correct operator\n                    correct_op_id = 12 if num1 < num2 else 13 if num1 > num2 else 14\n                    \n                    # Generate prediction\n                    try:\n                        generated = self.model.generate(input_seq)\n                        \n                        # Find generated operator\n                        gen_op = None\n                        for token in generated:\n                            if token in [12, 13, 14]:  # < or > or =\n                                gen_op = token\n                                break\n                                \n                        if gen_op is not None:\n                            total += 1\n                            if gen_op == correct_op_id:\n                                correct += 1\n                                \n                    except Exception as e:\n                        print(f\"Error during evaluation: {e}\")\n        \n        accuracy = correct / max(total, 1)\n        print(f\"Evaluation Results: {correct}/{total} correct = {accuracy:.4f} accuracy\")\n        return accuracy\n    \n    def fine_tune(self, train_dataloader, val_dataloader, num_epochs=3):\n        \"\"\"Fine-tune the model for multiple epochs.\"\"\"\n        # First evaluate the model\n        print(\"Initial evaluation:\")\n        initial_accuracy = self.evaluate(val_dataloader)\n        \n        best_accuracy = initial_accuracy\n        best_model_state = self.model.state_dict().copy()\n        \n        for epoch in range(num_epochs):\n            print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n            \n            # Train for one epoch\n            reward, train_acc = self.train_one_epoch(train_dataloader)\n            \n            # Evaluate\n            val_accuracy = self.evaluate(val_dataloader)\n            \n            # Save best model\n            if val_accuracy > best_accuracy:\n                best_accuracy = val_accuracy\n                best_model_state = self.model.state_dict().copy()\n                print(f\"New best model with accuracy: {best_accuracy:.4f}\")\n        \n        # Restore best model\n        self.model.load_state_dict(best_model_state)\n        print(f\"Fine-tuning complete. Best accuracy: {best_accuracy:.4f}\")\n        return best_accuracy\n    \n    def save_model(self, path):\n        \"\"\"Save the fine-tuned model.\"\"\"\n        torch.save(self.model.state_dict(), path)\n\n# Run improved fine-tuning\ndef run_improved_finetuning():\n    # Start with a fresh copy of the original pre-trained model\n    model = DecoderOnlyTransformer.load_from_checkpoint(\n        \"/kaggle/working/base.ckpt\",\n       num_tokens=19, \n        d_model=32, \n        num_heads=4, \n        num_layers=8, \n        d_ff=1024, \n        max_len=50, \n        dropout=0.1\n    )\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    # Create datasets with a variety of comparison examples\n    print(\"Generating training data...\")\n    train_data = generate_synthetic_data(num_data=5000, max_digits=3)\n    train_tensors = [torch.tensor(seq, dtype=torch.long) for seq in train_data]\n    train_dataloader = DataLoader(train_tensors, batch_size=64, shuffle=True, collate_fn=collate_fn)\n    \n    print(\"Generating validation data...\")\n    val_data = generate_synthetic_data(num_data=500, max_digits=3)\n    val_tensors = [torch.tensor(seq, dtype=torch.long) for seq in val_data]\n    val_dataloader = DataLoader(val_tensors, batch_size=64, shuffle=False, collate_fn=collate_fn)\n    \n    # Create fine-tuner\n    finetuner = ImprovedRLFineTuner(model, learning_rate=5e-6)\n    \n    # Fine-tune\n    finetuner.fine_tune(train_dataloader, val_dataloader, num_epochs=3)\n    \n    # Save model\n    finetuner.save_model(\"/kaggle/working/true_rl.pt\")\n    \n    # Test examples\n    test_examples = [\n        # [BOS]212.91or211.19[SEP]\n        [15, 2, 1, 2, 10, 9, 1, 11, 2, 1, 1, 10, 1, 9, 17],\n        \n        # [BOS]8.1or2.95[SEP]\n        [15, 8, 10, 1, 11, 2, 10, 9, 5, 17],\n        \n        # [BOS]125or115[SEP]\n        [15, 1, 2, 5, 11, 1, 1, 5, 17],\n        \n        # [BOS]21.01or21.02[SEP]\n        [15, 2, 1, 10, 0, 1, 11, 2, 1, 10, 0, 2, 17]\n    ]\n    \n    # Test the model on examples\n    print(\"\\nTesting the improved fine-tuned model:\")\n    for i, input_sequence in enumerate(test_examples):\n        generated_sequence = model.generate(input_sequence)\n        \n        # Print results\n        id_to_token = finetuner.id_to_token\n        input_text = \"\".join([id_to_token[token_id] for token_id in input_sequence])\n        generated_text = \"\".join([id_to_token[token_id] for token_id in generated_sequence])\n        \n        # Parse numbers for expected result\n        num1, num2 = finetuner.parse_numbers(input_sequence)\n        if num1 is not None and num2 is not None:\n            op = '<' if num1 < num2 else '>' if num1 > num2 else '='\n            expected = f\"{num1}{op}{num2}\"\n        else:\n            expected = \"Unable to parse\"\n        \n        print(f\"Example {i+1}:\")\n        print(f\"Input: {input_text}\")\n        print(f\"Generated: {generated_text}\")\n        print(f\"Expected: {expected}\")\n        print(\"-\" * 50)\n    \n    return model\n\n# Run the improved fine-tuning\nimproved_model = run_improved_finetuning()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:26:14.775331Z","iopub.execute_input":"2025-03-20T18:26:14.775645Z"}},"outputs":[{"name":"stderr","text":"INFO: Seed set to 42\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Seed set to 42\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Seed set to 42\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Generating training data...\nGenerating validation data...\nInitial evaluation:\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 8/8 [00:28<00:00,  3.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"Evaluation Results: 301/500 correct = 0.6020 accuracy\n\nEpoch 1/3:\n","output_type":"stream"},{"name":"stderr","text":"Training:   9%|▉         | 7/79 [00:34<05:48,  4.84s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}